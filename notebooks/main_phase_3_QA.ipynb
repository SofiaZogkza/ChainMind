{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "825d876b",
   "metadata": {},
   "source": [
    "# Phase 3: QA Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9503659c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI version: 2.8.1\n",
      "Pinecone version: 8.0.0\n",
      "Python version: 3.13.7 (main, Aug 14 2025, 11:12:11) [Clang 17.0.0 (clang-1700.0.13.3)]\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import pinecone\n",
    "import sys\n",
    "\n",
    "print(\"OpenAI version:\", openai.__version__)\n",
    "print(\"Pinecone version:\", pinecone.__version__)\n",
    "print(\"Python version:\", sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ba44a7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sofiazogkza/repos/Ironhack/ChainMind/venvSofia/bin/pip: line 2: /Users/sofiazogkza/repos/Ironhack/Ironhack Final Project/venvSofia/bin/python3.13: No such file or directory\n",
      "/Users/sofiazogkza/repos/Ironhack/ChainMind/venvSofia/bin/pip: line 2: exec: /Users/sofiazogkza/repos/Ironhack/Ironhack Final Project/venvSofia/bin/python3.13: cannot execute: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b88b578",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from pinecone import Pinecone\n",
    "import os\n",
    "\n",
    "# ------------------------------\n",
    "# Environment variables\n",
    "# ------------------------------\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "PINECONE_ENV = os.getenv(\"PINECONE_ENV\", \"us-east-1\")  # not used by new SDK but kept\n",
    "index_name = \"youtube-chunks\"\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# ------------------------------\n",
    "# Pinecone initialization\n",
    "# ------------------------------\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# ------------------------------\n",
    "# Conversation memory\n",
    "# ------------------------------\n",
    "conversation_history = []  # stores last turns\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# 1) Retrieve top chunks from pinecone\n",
    "# ------------------------------\n",
    "def retrieve_from_pinecone(query, k=3):\n",
    "    query_embedding = client.embeddings.create(\n",
    "        model=\"text-embedding-3-small\",\n",
    "        input=query\n",
    "    ).data[0].embedding\n",
    "\n",
    "    results = index.query(\n",
    "        vector=query_embedding,\n",
    "        top_k=k,\n",
    "        include_metadata=True\n",
    "    )\n",
    "\n",
    "    contexts = [m.metadata[\"text_chunk\"] for m in results.matches]\n",
    "    return \"\\n\\n\".join(contexts)\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# 2) LLM QA with memory\n",
    "# ------------------------------\n",
    "def answer_question(query):\n",
    "    # Get context from Pinecone\n",
    "    context = retrieve_from_pinecone(query)\n",
    "\n",
    "    # Build conversation memory (last 5 turns)\n",
    "    memory_text = \"\"\n",
    "    for turn in conversation_history[-5:]:\n",
    "        memory_text += f\"USER: {turn['question']}\\nASSISTANT: {turn['answer']}\\n\\n\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a helpful assistant answering questions about YouTube videos.\n",
    "\n",
    "You have TWO sources:\n",
    "\n",
    "1) Conversation Memory (for conversational continuity ONLY)\n",
    "{memory_text}\n",
    "\n",
    "2) Video Context (for factual information â€” ALWAYS use this for answering questions)\n",
    "{context}\n",
    "\n",
    "QUESTION: {query}\n",
    "\n",
    "If the answer is not in the context but is in the conversation memory, use the memory.\n",
    "If it is in neither, say: \"The video does not explain this clearly.\"\n",
    "\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.2,\n",
    "    )\n",
    "\n",
    "    answer = response.choices[0].message.content\n",
    "\n",
    "    # Save new turn to memory\n",
    "    conversation_history.append({\n",
    "        \"question\": query,\n",
    "        \"answer\": answer\n",
    "    })\n",
    "\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7c9a16",
   "metadata": {},
   "source": [
    "# ðŸ§ª TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "736d0984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How do I create a smart contract for a blockchain game?\n",
      "Answer: To create a smart contract for a blockchain game, you would need to follow these steps: \n",
      "1. Learn the basics of smart contracts.\n",
      "2. Set up a development environment.\n",
      "3. Create an NFT smart contract.\n",
      "4. Create a wagering smart contract.\n",
      "5. Deploy the smart contracts.\n",
      "6. Compile and display the smart contracts.\n"
     ]
    }
   ],
   "source": [
    "question = \"How do I create a smart contract for a blockchain game?\"\n",
    "answer = answer_question(question)\n",
    "print(\"Question:\", question)\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "704291fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Why should I create a game based on blockchain?\n",
      "Answer: The video does not explain this clearly.\n"
     ]
    }
   ],
   "source": [
    "question = \"Why should I create a game based on blockchain?\"\n",
    "answer = answer_question(question)\n",
    "print(\"Question:\", question)\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1361ea0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Give me an example!\n",
      "Answer: The video does not explain this clearly.\n"
     ]
    }
   ],
   "source": [
    "question = \"Give me an example!\"\n",
    "answer = answer_question(question)\n",
    "print(\"Question:\", question)\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b61c9f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: what was the previous example?\n",
      "Answer: The video does not explain this clearly.\n"
     ]
    }
   ],
   "source": [
    "question = \"what was the previous example?\"\n",
    "answer = answer_question(question)\n",
    "print(\"Question:\", question)\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ffc2bd",
   "metadata": {},
   "source": [
    "Result: \n",
    "\n",
    "- âŒ Hallucinations: Or GPT-3.5-turbo is too weak and ignores context when it's vague\n",
    "- We can observe that there is not memory to remember previous questions.\n",
    "\n",
    "Debug:\n",
    "- Added # DEBUG code in the previous codeblock & re-run the QAs.\n",
    "\n",
    "Solutions:\n",
    "- top_k from 3 to 34 (which are all the chunks).\n",
    "- added `summarize_chunk: Each chunk is summarized into 2-3 sentences.\n",
    "- summarizing each chunk on the fly -> But it made it veeeery SLOW. -> SOLUTION: Summarize chunks once | and store as `summar`y` in Pinecone metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20f5b9d",
   "metadata": {},
   "source": [
    "# ðŸ”— Setup LangChain + Pinecone retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "20f63229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -qU langchain-pinecone langchain-openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6bfd3e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from pinecone import Pinecone\n",
    "\n",
    "# Use your OpenAI API key\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Wrap Pinecone index as LangChain vectorstore\n",
    "vectorstore = PineconeVectorStore(\n",
    "    index=index,\n",
    "    embedding=embeddings,\n",
    "    text_key=\"text_chunk\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e852e9",
   "metadata": {},
   "source": [
    "### Create a **RetrievalQA** chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e198c4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To build a blockchain game, you can follow these steps:\n",
      "\n",
      "1. Start by using the third web CLI to create a smart contract for your game.\n",
      "2. Choose a smart contract framework like Hardhat or Forge.\n",
      "3. Develop the smart contract for your game, including functions for authentication, querying NFTs, depositing NFTs into a wagering smart contract, and withdrawing winnings.\n",
      "4. Implement the front end code to interact with the Ethereum blockchain.\n",
      "5. Create a user interface for wallet interaction.\n",
      "6. Test the game to ensure it functions properly.\n",
      "7. Promote the game to attract players.\n",
      "8. Consider giving away NFTs for free to promote real ownership in blockchain gaming.\n",
      "9. Make sure to focus on creating high-quality projects and changing the perception of blockchain gaming.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.2,\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "# Create retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# Define prompt template\n",
    "template = \"\"\"Answer the question based on the following context from video transcripts:\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Build the RAG chain - format docs from retriever properly\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | RunnableLambda(lambda docs: \"\\n\\n\".join([doc.page_content for doc in docs])),\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Use it to answer questions\n",
    "response = rag_chain.invoke(\"How to build a blockchain game?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c631e960",
   "metadata": {},
   "source": [
    "Observations:\n",
    "- `RetrievalQA` was deprecated in LangChain 1.0+ because it's less flexible than building chains manually with runnables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5f20950c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How to build a blockchain game?\n",
      "Answer: To build a blockchain game, you can follow these steps:\n",
      "\n",
      "1. Learn the basics of blockchain technology and smart contracts.\n",
      "2. Set up a development environment for blockchain development.\n",
      "3. Create an NFT smart contract for your game.\n",
      "4. Create a wagering smart contract for players to enter the game.\n",
      "5. Deploy the smart contracts to a local test blockchain.\n",
      "6. Implement the front end code to interact with the Ethereum blockchain.\n",
      "7. Create functions to authenticate users, query NFTs, deposit NFTs into the wagering smart contract, and withdraw winnings.\n",
      "8. Create a user interface for wallet interaction and game play.\n",
      "9. Test the game and promote it to users.\n",
      "\n",
      "By following these steps, you can create a blockchain game with real ownership and unique gameplay mechanics.\n"
     ]
    }
   ],
   "source": [
    "question = \"How to build a blockchain game?\"\n",
    "print(\"Question:\", question)\n",
    "print(\"Answer:\", end=\" \", flush=True)\n",
    "\n",
    "for chunk in rag_chain.stream(question):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9e04ca4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How to reate an NFT smart contract for my game?\n",
      "Answer: To create an NFT smart contract for your game, you can follow these steps:\n",
      "\n",
      "1. Learn the basics of smart contract development.\n",
      "2. Set up a development environment.\n",
      "3. Create the NFT smart contract using a tool like gbt4.\n",
      "4. Deploy the smart contract to a local test blockchain.\n",
      "5. Connect the NFT smart contract with your game to authenticate users, query NFTs, deposit NFTs into the wagering smart contract, and withdraw winnings.\n",
      "6. Implement the front end code to interact with the Ethereum blockchain.\n",
      "7. Test the game to ensure it functions properly.\n",
      "8. Create a user interface for wallet interaction.\n",
      "9. Promote the game to attract players.\n",
      "\n",
      "By following these steps, you can successfully create an NFT smart contract for your game.\n"
     ]
    }
   ],
   "source": [
    "question = \"How to reate an NFT smart contract for my game?\"\n",
    "print(\"Question:\", question)\n",
    "print(\"Answer:\", end=\" \", flush=True)\n",
    "\n",
    "for chunk in rag_chain.stream(question):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2288cc1f",
   "metadata": {},
   "source": [
    "# MEMORY + INTERNET SEARCH AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6bb279c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# INTERNET SEARCH AGENT WITH MEMORY + FOLLOW-UP HANDLING\n",
    "# ======================================================\n",
    "\n",
    "from openai import OpenAI\n",
    "from serpapi import GoogleSearch\n",
    "import json\n",
    "import os\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# ----------------------------\n",
    "# GLOBAL MEMORY\n",
    "# ----------------------------\n",
    "conversation_history = []\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# INTERNET SEARCH TOOL\n",
    "# ======================================================\n",
    "def internet_search(query):\n",
    "    params = {\n",
    "        \"q\": query,\n",
    "        \"api_key\": os.getenv(\"SERPAPI_KEY\")\n",
    "    }\n",
    "    search = GoogleSearch(params)\n",
    "    result = search.get_dict()\n",
    "\n",
    "    if \"organic_results\" in result and len(result[\"organic_results\"]) > 0:\n",
    "        return \"\\n\".join([item.get(\"snippet\", \"\") for item in result[\"organic_results\"][:3]])\n",
    "\n",
    "    return \"No results found.\"\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# MAIN AGENT FUNCTION\n",
    "# ======================================================\n",
    "def agent_with_search(query, rag_answer):\n",
    "    global conversation_history\n",
    "\n",
    "    # ----------------------------\n",
    "    # Build memory context text\n",
    "    # ----------------------------\n",
    "    memory_text = \"\"\n",
    "    for turn in conversation_history[-5:]:\n",
    "        memory_text += f\"USER: {turn['question']}\\nASSISTANT: {turn['answer']}\\n\\n\"\n",
    "\n",
    "    # ======================================================\n",
    "    # 0) FOLLOW-UP QUESTION DETECTION\n",
    "    # ======================================================\n",
    "    follow_up_phrases = [\n",
    "        \"give me an example\",\n",
    "        \"another example\",\n",
    "        \"what was the previous\",\n",
    "        \"explain again\",\n",
    "        \"repeat that\",\n",
    "        \"continue\",\n",
    "        \"elaborate\",\n",
    "        \"what did you say before\",\n",
    "        \"remind me\",\n",
    "        \"again please\"\n",
    "    ]\n",
    "\n",
    "    lower_q = query.lower()\n",
    "\n",
    "    # If this is a follow-up question AND we have memory â†’ return last answer\n",
    "    if any(phrase in lower_q for phrase in follow_up_phrases):\n",
    "        if len(conversation_history) > 0:\n",
    "            final_answer = conversation_history[-1][\"answer\"]\n",
    "\n",
    "            # Save follow-up question to memory too\n",
    "            conversation_history.append({\n",
    "                \"question\": query,\n",
    "                \"answer\": final_answer\n",
    "            })\n",
    "            return final_answer\n",
    "\n",
    "\n",
    "    # ======================================================\n",
    "    # 1) RAG KNOWS THE ANSWER â†’ ACCEPT IT\n",
    "    # ======================================================\n",
    "    if rag_answer.strip() != \"The video does not explain this clearly.\":\n",
    "        conversation_history.append({\n",
    "            \"question\": query,\n",
    "            \"answer\": rag_answer\n",
    "        })\n",
    "        return rag_answer\n",
    "\n",
    "\n",
    "    # ======================================================\n",
    "    # 2) RAG FAILED â†’ TRY INTERNET SEARCH\n",
    "    # ======================================================\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"internet_search\",\n",
    "                \"description\": \"Search the web for real-time information.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"query\": {\"type\": \"string\"}\n",
    "                    },\n",
    "                    \"required\": [\"query\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Ask the model whether to use the tool\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\",\n",
    "             \"content\": (\n",
    "                 \"You are an intelligent agent.\\n\"\n",
    "                 \"Use MEMORY for context, but if the video lacks the answer, \"\n",
    "                 \"call the internet_search tool.\\n\"\n",
    "             )},\n",
    "            {\"role\": \"assistant\", \"content\": f\"MEMORY:\\n{memory_text}\"},\n",
    "            {\"role\": \"user\", \"content\": query}\n",
    "        ],\n",
    "        tools=tools,\n",
    "        tool_choice=\"auto\"\n",
    "    )\n",
    "\n",
    "    message = response.choices[0].message\n",
    "\n",
    "    # ======================================================\n",
    "    # 3) TOOL CALL â†’ INTERNET SEARCH\n",
    "    # ======================================================\n",
    "    if message.tool_calls:\n",
    "        tool_call = message.tool_calls[0]\n",
    "        args = json.loads(tool_call.function.arguments)\n",
    "        search_query = args.get(\"query\", query)\n",
    "\n",
    "        search_output = internet_search(search_query)\n",
    "\n",
    "        # Now rewrite search results into a helpful answer\n",
    "        refined = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Rewrite the search results into a clean, helpful answer.\"},\n",
    "                {\"role\": \"assistant\", \"content\": f\"Search results:\\n{search_output}\"},\n",
    "                {\"role\": \"user\", \"content\": query}\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        final_answer = refined.choices[0].message.content\n",
    "\n",
    "        # Save memory\n",
    "        conversation_history.append({\n",
    "            \"question\": query,\n",
    "            \"answer\": final_answer\n",
    "        })\n",
    "\n",
    "        return final_answer\n",
    "\n",
    "    # ======================================================\n",
    "    # 4) NO TOOL CALL â†’ fallback to RAG answer\n",
    "    # ======================================================\n",
    "    conversation_history.append({\n",
    "        \"question\": query,\n",
    "        \"answer\": rag_answer\n",
    "    })\n",
    "    return rag_answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "993218de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How do I create a smart contract for a blockchain game?\n",
      "RAG Answer: MEMORY: To create a smart contract for a blockchain game, you can use the third web CLI to create a smart contract as the backend engine for your game. Choose a smart contract framework like Hardhat or Forge, and follow step-by-step instructions to create the necessary components such as NFT smart contracts and wagering smart contracts. Additionally, deploy the smart contracts to a local test blockchain and implement front-end code to interact with the Ethereum blockchain. This process will help you get started on creating a smart contract for a blockchain game.\n",
      "\n",
      "Agent Answer: MEMORY: To create a smart contract for a blockchain game, you can use the third web CLI to create a smart contract as the backend engine for your game. Choose a smart contract framework like Hardhat or Forge, and follow step-by-step instructions to create the necessary components such as NFT smart contracts and wagering smart contracts. Additionally, deploy the smart contracts to a local test blockchain and implement front-end code to interact with the Ethereum blockchain. This process will help you get started on creating a smart contract for a blockchain game.\n"
     ]
    }
   ],
   "source": [
    "query = \"How do I create a smart contract for a blockchain game?\"\n",
    "print(\"Question:\", query)\n",
    "# 1. Get the RAG answer first\n",
    "rag_answer = answer_question(query)\n",
    "print(\"RAG Answer:\", rag_answer)\n",
    "\n",
    "# 2. Use the agent to decide whether to search the internet\n",
    "final_answer = agent_with_search(query, rag_answer)\n",
    "print(\"\\nAgent Answer:\", final_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "73672817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Why should I create a game based on blockchain?\n",
      "\n",
      "RAG Answer: MEMORY: Creating a game based on blockchain allows for real ownership of in-game assets, ensuring that players truly own the items they acquire in the game. This can help combat scams and provide a new level of transparency and security in gaming.\n",
      "\n",
      "Agent Answer: MEMORY: Creating a game based on blockchain allows for real ownership of in-game assets, ensuring that players truly own the items they acquire in the game. This can help combat scams and provide a new level of transparency and security in gaming.\n"
     ]
    }
   ],
   "source": [
    "query = \"Why should I create a game based on blockchain?\"\n",
    "print(\"Question:\", query)\n",
    "\n",
    "rag_answer = answer_question(query)\n",
    "print(\"\\nRAG Answer:\", rag_answer)\n",
    "final_answer = agent_with_search(query, rag_answer)\n",
    "print(\"\\nAgent Answer:\", final_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a19376d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Give me an example!\n",
      "\n",
      "RAG Answer: MEMORY: The previous example was about customizing a smart contract for a blockchain game to change the logic of burning NFTs and minting new tokens when a specific action is performed.\n",
      "\n",
      "Agent Answer: MEMORY: The previous example was about customizing a smart contract for a blockchain game to change the logic of burning NFTs and minting new tokens when a specific action is performed.\n"
     ]
    }
   ],
   "source": [
    "query = \"Give me an example!\"\n",
    "print(\"Question:\", query)\n",
    "rag_answer = answer_question(question)\n",
    "print(\"\\nRAG Answer:\", rag_answer)\n",
    "final_answer = agent_with_search(question, rag_answer)\n",
    "print(\"\\nAgent Answer:\", final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a18a222b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What was the previous example?\n",
      "\n",
      "RAG Answer: MEMORY: The previous example was about customizing a smart contract for a blockchain game to change the logic of burning NFTs and minting new tokens when a specific action is performed.\n",
      "\n",
      "Agent Answer: MEMORY: The previous example was about customizing a smart contract for a blockchain game to change the logic of burning NFTs and minting new tokens when a specific action is performed.\n"
     ]
    }
   ],
   "source": [
    "query = \"What was the previous example?\"\n",
    "print(\"Question:\", query)\n",
    "rag_answer = answer_question(question)\n",
    "print(\"\\nRAG Answer:\", rag_answer)\n",
    "final_answer = agent_with_search(question, rag_answer)\n",
    "print(\"\\nAgent Answer:\", final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5151317d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is wagering smart contract?\n",
      "\n",
      "RAG Answer: MEMORY: The wagering smart contract for a game interacts with the previously created NFT contract and allows players to deposit their tokens, create matches, determine winners and losers, and transfer tokens to the winner.\n",
      "\n",
      "Agent Answer: MEMORY: The wagering smart contract for a game interacts with the previously created NFT contract and allows players to deposit their tokens, create matches, determine winners and losers, and transfer tokens to the winner.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is wagering smart contract?\"\n",
    "print(\"Question:\", query)\n",
    "\n",
    "rag_answer = answer_question(query)\n",
    "print(\"\\nRAG Answer:\", rag_answer)\n",
    "final_answer = agent_with_search(query, rag_answer)\n",
    "print(\"\\nAgent Answer:\", final_answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvSofia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
